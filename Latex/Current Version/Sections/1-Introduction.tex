\section{Introduction}
At the heart of several problems in optimisation, statistics, and machine learning lies the task of identifying a point that satisfies multiple constraints simultaneously. When these constraints correspond to closed convex sets in a Hilbert space, this task takes the form of the Best Approximation Problem (BAP).
%
Formally, we denote as $X$ a real Hilbert space equipped with an inner product $\langle \cdot, \cdot \rangle$ and its induced norm $\| \cdot \|$. Consider a finite family of closed convex subsets $\{\mathcal{H}_i\}_{i=1}^m$ of $X$ with a non-empty intersection $\mathcal{H} := \bigcap_{i=1}^m \mathcal{H}_i$. Given an arbitrary point $x^\circ \in X$, the solution to the BAP is the unique point $x^* \in \mathcal{H}$ closest to $x^\circ$ with respect to the norm $\| \cdot \|$. This solution, denoted as $x^* := P_{\mathcal{H}}(x^\circ)$, has guaranteed its existence and uniqueness by virtue of the convexity of $\mathcal{H}$, and is found as the solution to the following convex optimisation problem

\begin{equation} \label{eq:bap}
    \begin{array}{ll}
        \text{minimise} & \frac{1}{2} \|x - x^\circ\|^2  \\
        \text{subject to} & x \in \mathcal{H}.
    \end{array}
\end{equation}

The BAP formulation in \eqref{eq:bap} is widely adopted in applications such as restricted least squares and isotonic regression in statistics, as well as in signal and image processing, where convex sets encode prior knowledge, like non-negativity or sparsity, and in machine learning, where solutions to learning problems must often adhere to structural constraints. When the space $X$ is Euclidean, the BAP takes the form of a constrained quadratic program (CQP). The primary computational challenge in solving~\eqref{eq:bap} is that the projection operator $P_{\mathcal{H}}$ onto the intersection set is typically intractable, even when the projections $P_{\mathcal{H}_i}$ onto each individual constraint set $\mathcal{H}_i$ are simple to compute (e.g., for half-spaces or Euclidean balls). State-of-the-art CQP solvers typically address this challenge by augmenting the problem with additional variables to absorb the constraints, which can reduce computational efficiency~\cite{KEMPF20206542}. In contexts where the projection is part of a larger iterative process, this inefficiency can render the optimal solution too expensive to compute, particularly in high-dimensional or time-sensitive applications.

As a recent example, Diamond Light Source (DLS), the UK's national synchrotron, is expected to see an upgrade to its electron beam stabilisation control system. Existing control methods (modal decomposition~\cite{HEATH}) are no longer sufficient: this is due to the increased number of sensors (from 172 to 252) and actuators (from 173 to 396), as well as the introduction of two different types of corrector magnets. A devised model predictive control (MPC) implementation~\cite{MPCDLSII}, while promising, faces high computational demands for high-frequency (100 kHz) control. To accommodate input rate constraints, the Fast Gradient method was proposed in~\cite{KEMPF20206542}, where a CQP problem needs to be solved at each timestep as part of the main loop, which is to be run less than \SI{10}{\micro\second} for feasibility. Evaluating the solution to the CQP using standard solvers would be too expensive for this high-latency requirement. This difficulty necessitates iterative methods that construct the solution to the CQP in linear time using only the individual projection operators $P_{\mathcal{H}_i}$. Amongst these, Dykstra's method offers a promising solution due to its simplicity and convergence guarantees.

Dykstra's algorithm~\cite{DYKSTRA} is an iterative method designed to find a solution to the Euclidean BAP when the sets $\mathcal{H}_i$ are convex.
It is a variant of von Neumann's Method of Alternating Projections (MAP) that uses auxiliary variables to achieve strong convergence to the specific projection of the initial point onto the intersection of convex sets. This is a fundamental advantage over MAP, which only guarantees convergence to some arbitrary point within the intersection.
Dykstra's algorithm achieves this by using auxiliary variables that, qualitatively, ``remember" the direction vector of projection; these are added to the primary variables before subsequent projections and, as a result, the primary iterates of Dykstra's method asymptotically converge to the ``true" projection. This renders Dykstra's method a highly valuable algorithm to deploy in real-time, high-frequency applications for finding fast approximate solutions to BAPs. The implementation of Dykstra's method can significantly reduce time complexity in applications such as that of DLS~\cite{KEMPF20206542}. However, despite its strong theoretical guarantees, Dykstra's algorithm is known to suffer from a \emph{stalling} problem for certain classes of initial conditions~\cite{DYKSTRASTALLING}. In such cases, the iterates of Dykstra's method remain unchanged for a number of iterations. The duration of the stalling period cannot be determined a priori, and, given the choice of starting point, can be made arbitrarily long. Although a robust stopping criterion can resolve this last issue, in general, the algorithm cannot be run for a fixed number of iterations with a guarantee that the output $x^*$ will be closer to the projection than the initial point $x^\circ$. This phenomenon strongly hinders the practical applicability of Dykstra's method.

This paper solves the stalling problem \cite{DYKSTRASTALLING} for polyhedral sets by (1) formulating a definition of the length of the stalling period (Definition~\ref{def:stalling}), (2) deriving a closed-form theorem (Theorem~\ref{thm:nstall}) to calculate its exact length a priori, and (3) proposing a ``fast-forward" modification to the algorithm that eliminates stalling. Our solution preserves the original algorithm's convergence guarantees.

The structure of the paper is as follows. Sections~\ref{subsec:euclidean_projections}-\ref{subsec:admm_connection} review the current literature on iterative projection schemes and introduce Dykstra's algorithm. Section~\ref{sec:polyhedral_case}-\ref{subsec:stalling} narrow the analysis to polyhedral sets and introduce the stalling phenomenon. Sections~\ref{sec:main_result} introduces the fast-forward modification and presents numerical experiments demonstrating the effectiveness of the solution, and it provides a control-theoretical perspective on the method. Finally, Section~\ref{sec:conclusion} concludes with a discussion of potential accelerations, extensions, and applications. 