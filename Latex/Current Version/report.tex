\documentclass[hidelinks]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START CUSTOM INCLUDES & DEFINITIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath,amssymb,amsfonts,amsthm}
%\usepackage{parskip} %noident everywhere
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{overpic}
\usepackage{mymath}
\usepackage{nth}
\usepackage{caption}
\usepackage{todonotes}
\usepackage{fullpage}
\usepackage{arydshln} % dashed line in array
\usepackage{MnSymbol} % anti-diag dots
%\usepackage{showlabels} % show equation and figure labels
\usepackage{subcaption}
\usepackage{varwidth, tikz}
\usetikzlibrary{shapes, arrows, shapes.misc, arrows.meta, positioning, matrix, calc, fit, fadings, patterns}
\usetikzlibrary{calc,patterns,decorations.pathmorphing,decorations.markings,arrows, arrows.meta}
\usepackage[export]{adjustbox}
\usepackage{placeins}
%\setlength{\parindent}{0pt} 
\usepackage{tabto}

\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% ALG STUFF
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\algorithmicbreak}{\textbf{break}}
\newcommand{\Break}{\State \algorithmicbreak}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\makeatletter
\newcommand{\algrule}[1][.2pt]{{\color{black!10!white}{\par\vskip.1\baselineskip\hrule height #1\par\vskip.1\baselineskip}}}
\makeatother
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END CUSTOM INCLUDES & DEFINITIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pdfobjcompresslevel=0

\title{Fast-Forwarding Stalling of Dykstra's Alternating Projection for Polyhedral Sets\thanks{This research was supported by the Keble College Small Research Grant (KSRG118).}}
\author{Claudio Vestini and Idris Kempf}
\date{Michaelmas Term 2024}
\begin{document}

\thispagestyle{empty}

\noindent
\textit{
The Keble Small Research Grant (KSRG) KSRG118 was used to remunerate Claudio Vestini, a third-year engineering science undergraduate at Keble, for his work over seven weeks during the long vacation. The majority of the technical work was carried out by Claudio, and weekly meetings were held throughout the project. The KSRG successfully enabled me to launch a new research stream, and the research outputs will be incorporated into a future publication with Claudio as an author, in which KSRG support will be gratefully acknowledged. Additionally, the program code developed during this project will be made publicly available upon publication.
}


\section*{Summary}
Euclidean projections are ubiquitous in engineering, optimisation, and machine learning, playing a central role in many algorithms for solving problems constrained by complex sets. Mathematically, the projection of a point $x^\circ$ onto a set $S$ is defined as the closest point $x^\star\in S$ to $x^\circ$, as illustrated in Figure~\ref{fig:proj} for two dimensions. Graphically, this definition can be extended to the projection of a set of points onto another set to obtain the shadow of an object in 3D. In constrained optimisation problems, this definition is extended to $N$ dimensions, modelling the projection of optimisation variables onto a constraint set. For example, the constraint set could represent delivery stops and available trucks in an optimisation problem that aims to minimise delivery times and fuel consumption. Such an optimisation problem is typically solved using a \textit{solver} -- an algorithm that iteratively approaches an optimum while projecting the variables onto a constraint set. However, the projection itself defines another optimisation problem that is difficult to solve in general. Most solvers therefore simplify the projection by reformulating the optimisation problem and adding auxiliary variables related to the constraints, usually at the expense of accuracy and computational efficiency.

\begin{figure}[!h]
\centering
\begin{tikzpicture}[scale=0.8]
    % Draw the potato-shaped set S
    \draw[thick, fill=blue!20, opacity=0.5, rounded corners=1pt] 
        plot[smooth cycle, tension=0.8] coordinates {(1,1) (2,0.8) (3,1.2) (3.5,2) (2.8,3) (1.5,2.5)};
    \node[above right] at (1,1) {$S$};

    % Define coordinates
    \coordinate (x0) at (0,3.5); % x^\circ
    \coordinate (x_star) at (1.375,2.375); % x^* (on the potato boundary)

    % Draw the projection line
    \draw[thick, dashed] (x0) -- (x_star);

    % Draw points
    \filldraw[black] (x0) circle (1.5pt) node[above right] {$x^\circ$};
    \filldraw[black] (x_star) circle (1.5pt) node[below right] {$x^\star$};
\end{tikzpicture}
\caption{Illustration of the two-dimensional projection $x^\star$ of a point $x^\circ$ onto a set $S$.}\label{fig:proj}
\end{figure}

An alternative to solving the complicated projection problem could be to use a specialised algorithm, \textit{Dykstra's Alternating Projection}, an iterative procedure developed in the 1980s. However, although Dykstra's algorithm has proven to converge in theory, several practical issues remain. For example, it has been shown that the algorithm can \textit{stall} in certain situations for an indefinite number of iterations, yielding inaccurate solutions or decreasing computational efficiency. In this project, we successfully addressed and solved the stalling problem associated with projections onto polyhedral sets, i.e. an intersection of multi-variable (in)equalities each of which can be interpreted as an $N$-dimensional half-space. Polyhedral sets can be used to approximate most constraint sets in practice and therefore play a particularly important role in engineering and optimisation.

To solve this problem, stalling situations referenced in the literature were replicated both graphically and mathematically. The mathematical properties of polyhedral sets were then used to simplify Dykstra's algorithm. Following an analysis of the simplified algorithm during stalling scenarios, a procedure for accurately predicting the \textit{length} of the stalling period was identified. This procedure was then embedded in the algorithm to ``fast-forward'' the stalling period. These results are summarised in the attached technical report, which will serve as a draft for a future publication. This publication will also address additional research questions, such as a detailed characterisation of the convergence properties of the modified algorithm. The fast-forwarding approach could also be applied to similar algorithms that have been shown to have stalling issues.

\newpage

\maketitle
\section{Introduction}

The projection of a point $x^\circ\in\R^n$ onto a closed convex set $\mathcal{H}\subset \R^p$ is defined as the point $x^\star\eqdef \mathcal{P}_{\mathcal{H}}(x)$ that minimises the Euclidean distance between $x^\circ$ and any point in $\mathcal{H}$:
\begin{align}\label{eq:projection}
\mathcal{P}_{\mathcal{H}}(x)\eqdef \argmin_{x\in\mathcal{H}}\twonorm{x-x^\circ}.
\end{align}
By the convexity of the set $\mathcal{H}$, problem~\eqref{eq:projection} admits a unique solution~\cite[Ch.\ 3.2]{BAUSCHKEBOOK}. Here, we assume that $\mathcal{H}$ is a convex polyhedral set that can be represented as
\begin{align}\label{eq:polyhedron}
\mathcal{H}\eqdef\set{x\in\R^p}{A x \leq c},
\end{align}
where $\inR{A}{n}{p}$ and $c\in\R^n$. Each row of $\mathcal{H}$ corresponds to a half-space $\mathcal{H}_i$,
\begin{align}\label{eq:sets_i}
\mathcal{H}_i\eqdef\set{x\in\R^p}{\trans{f_i}x \leq c_i},
\end{align}
where $i=1,\dots,n$ and $f_i$, $\twonorm{f_i}=1$, is the normal vector of the plane $H_i\eqdef\set{x\in\R^p}{\trans{f_i}x = c_i}$. The polyhedral set~\eqref{eq:polyhedron} can also be represented as the intersection of the half-spaces $\mathcal{H}_i$, i.e.\ $\mathcal{H}=\bigcap_{i=0}^{n-1}\mathcal{H}_i$. In many engineering applications, constraint sets can be approximated using~\eqref{eq:polyhedron}.

While for some sets the solution of~\eqref{eq:polyhedron} can be obtained explicitly and in closed form, e.g., when $\mathcal{H}$ is an $n$-dimensional cube, no closed-form solution is known for arbitrary polyhedral sets. In these cases, the solution can be obtained using a solver for constrained quadratic programs. To solve~\eqref{eq:projection}, most solvers require introducing an additional variable $z\eqdef Ax$ that is projected onto the set $\set{z\in\R^n}{z \leq c}$, for which an explicit solution exists. However, for large $n$, this variable augmentation can reduce the computational efficiency, in particular when the projection is part of a larger iterative algorithm.

As an alternative to variable augmentation, problem~\eqref{eq:projection} can be solved using \emph{Dykstra's Alternating Projection Algorithm}~\cite{DYKSTRA}. Dykstra's algorithm, first published in 1983, extends von Neumann's \emph{Method of Alternating Projections} (MAP)~\cite{NEUMANN}, which is designed to find a point lying in the intersection of $n$ closed convex sets by cyclically projecting onto each individual set. While von Neumann's algorithm identifies \emph{some} point in the intersection, Dykstra's algorithm determines the Euclidean projection~\eqref{eq:projection}. Both algorithms circumvent the potentially complex projection $\mathcal{P}_\mathcal{H}$ by iteratively applying the (known) projections $\mathcal{P}_{\mathcal{H}_0},\dots,\mathcal{P}_{\mathcal{H}_{n-1}}$. 

Although Dykstra's algorithm is proven to eventually converge to the projection, it has shown to be prone to \emph{stalling}~\cite{DYKSTRASTALLING} for certain sets, including polyhedral sets like~\eqref{eq:polyhedron}. In such cases, the iterates of Dykstra's method remain unchanged for a number of iterations. The duration of the stalling period cannot be determined \emph{a priori}, and depending on the starting point, can be arbitrary long. This phenomenon hinders the application of Dykstra's method in practice. Additionally, the algorithm \emph{cannot} be run for a fixed number of iterations with a guarantee that the output will be closer to the projection than the initial point -- a guarantee typically required when embedding Dykstra's method in iterative schemes, such as gradient methods~\cite{FGMPROJECTION}.

\section{Dykstra's Alternating Projection}\label{sec: dykstra}

Given $n$ convex sets $\mathcal{H}_0,\dots,\mathcal{H}_{n-1}$, Dykstra's alternating projection algorithm~\cite{DYKSTRAPERKINS} finds the orthogonal projection $x^\star$ of $x^\circ$ onto $\mathcal{H}\eqdef{\bigcap}_{i=0}^{n-1} \mathcal{H}_i$ by generating a series of iterates \{$x_{m}$\} using the scheme
\begin{subequations}\label{eq:dykstra}
\begin{align}
x_{m+1}&=\mathcal{P}_{\mathcal{H}_{[m]}}\left(x_{m}+e_{m-n}\right),\label{eq:dykstra:proj}\\
e_m&=e_{m-n}+x_{m}-x_{m+1}\label{eq:dykstra:error},
\end{align}
\end{subequations}
where $[m]\eqdef m \, \text{mod} \, n$, $x_0=x$, $\mathcal{P}_{[m]}\eqdef\mathcal{P}_{\mathcal{H}_{[m]}}$, and the auxiliary variables $e_m$ are initialised as
\begin{align}\label{eq:initial error}
&e_{-n} = e_{-(n-1)} = ... = e_{-1} = 0.
\end{align}

Note that von Neumann's MAP can be obtained from~\eqref{eq:dykstra} by setting $e_m \equiv 0\quad\forall m$. The Boyle-Dykstra theorem~\cite{DYKSTRA} implies that $\lim_{m\rightarrow\infty}\anynorm{x_m-\mathcal{P}_\mathcal{H}(x)}=0$. For a finite number of iterations, there is no guarantee that $x_m\in\mathcal{H}$ nor that $x_m\neq x$. 

\subsection{The Polyhedral Case}

For polyhedral sets~\eqref{eq:sets_i}, the projection step~\eqref{eq:dykstra:proj} can be simplified to
\begin{align}\label{eq:dykstra:proj:poly}
x_{m+1}=
\begin{cases}
x_{m}+e_{m-n} & \text{if } x_{m}+e_{m-n}\in\mathcal{H}_{[m]}\\
x_{m}+e_{m-n} - \left((x_{m}+e_{m-n})^\Tr f_{[m]} - c_{[m]}\right) f_{[m]} & \text{if } x_{m}+e_{m-n}\not\in\mathcal{H}_{[m]}
\end{cases},
\end{align}
and the update for the auxiliary vector to
\begin{align}\label{eq:dykstra:error:poly}
e_m=
\begin{cases}
0 & \text{if } x_{m}+e_{m-n}\in\mathcal{H}_{[m]},\\
\left((x_{m}+e_{m-n})^\Tr f_{[m]} - c_{[m]}\right) f_{[m]} & \text{if } x_{m}+e_{m-n}\not\in\mathcal{H}_{[m]},
\end{cases}.
\end{align}
The auxiliary vector $e_m$ is either $0$ or parallel to $f_{[m]}$, so that it can be represented as $e_m = k_m f_{[m]}$ with $k_m=\text{dist}_{\mathcal{H}_{[m]}}(x_{m-1}+e_{m-n})$, further simplifying~\eqref{eq:dykstra:error:poly} to
\begin{align}\label{eq:km}
k_m = k_{m-n} + x_m^\Tr f_{[m]} - c_{[m]}.
\end{align}

The convergence of Dykstra's iterates to the Eucledian projection has been analysed in~\cite{DYKSTRAPOLY2,DYKSTRAPOLY,DYKSTRAPERKINS} for polyhedral sets. The proof is based on partitioning the sets into inactive ($x^\star\not\in H_i$) and active sets ($x^\star\in H_i$), i.e.\
\begin{align}
&A=\set{i\in\lbrace 0,\dots,n-1\rbrace}{x_\infty\in H_i},
&B=\lbrace 0,\dots,n-1\rbrace\backslash A,
\end{align}
where $x_\infty=\lim_{m\rightarrow\infty}x_m$. It can be shown that there exists a number $N_1$ such that whenever
\begin{align}
[m]\in B,\quad m\geq N_1\quad\Rightarrow\quad x_m=x_{m-1},\quad e_m=0,
\end{align}
i.e. the half-spaces that become ``inactive'' remain inactive. Furthermore, there exists $N_2\geq N_1$ such that whenever $n\geq N_2$, it holds that
\begin{align}\label{eq:dykstra:error:poly}
\twonorm{x_{m+n}-x_\infty}\leq\alpha_{[m]}\twonorm{x_m-x_\infty},
\end{align}
where $0\leq\alpha_{[m]}<1$ are numbers related to angles between half-spaces. The number $N_2$ describes the iteration from which on the algorithm has determined the inactive half-spaces. Finally, it is shown that the iterates of the algorithm satisfy the following inequality:
\begin{theorem}[Deutsch and Hundal~\cite{DYKSTRAPOLY}]
There exist constants $0\leq c < 1$ and $\rho > 0$ such that
\begin{align*}
\anynorm{x_m -x_\infty} \leq \rho c^m.
\end{align*}
\end{theorem}
The factor $c$ can be estimated from the smallest $\alpha_{[m]}$, which is characterized by the angle between certain subspaces (subspaces formed by the ``active'' halfspaces). The factor $\alpha_{[m]}$ can be upper-bounded by considering the ``worst'' angles in the polyhedron. The constant $\rho$, however, depends on an unknown iteration number $N_3\geq N_2$ and on the starting point $x^\circ$, and can therefore not be computed in advance~\cite{DYKSTRAPERKINS,XUPOLY}. In the case of stalling, the variable $\rho$ can become arbitrarily large, making the application of Dykstra's method difficult in practice. The authors of~\cite{DYKSTRAPERKINS} proposed a combined Dysktra-conjugate-gradient method that allows for computing an upper bound on $\anynorm{x_m -x_\infty}$. The authors of~\cite{XUPOLY} proposed an alternative algorithm called \emph{successive approximate algorithm}, which promises fast convergence, conditioned on knowing a point $x\in\mathcal{H}$ in advance.

%\begin{remark}[Random thoughts]
%Dykstra's method could be interpreted as an autonomous non-linear discrete-time system with initial condition $x^\circ$. For $m\geq N_2$, I believe that the discrete-time system becomes \emph{linear} and could be reformulated as $x_{m+n} = G x_m + b$.
%Since by the Boyle-Dykstra theorem $x_{m+n}$ necessarily converges to the projection for $n\rightarrow\infty$, the matrix $G$ must be Schur stable, so that the projection can be obtained from $x^\star = \inv{\left(I-G\right)} b$. The question is whether $N_2$ can be detected (may have been answered in the literature already).
%\end{remark}

\subsection{Stalling}

In~\cite{DYKSTRASTALLING}, the behaviour of Dykstra's method is analysed for two sets. The authors give conditions on Dykstra's algorithm for (i) finite convergence, (ii) infinite convergence, and (iii) stalling followed by infinite convergence. A specific example is given for the case that the set is provided by the intersection of a line with a unit box in $\R^2$ ($\mathcal{H}$ is a polyhedron). It can be shown that cases (i)--(iii) depend on the starting point $x_0$, and one can determine the 3 regions shown in Figure~\ref{fig:region} that yield different convergence behaviour. Convergence case (i) is obtained when starting in the green region, case (ii) when starting in the blue region, and case (iii) when starting in the red region.

To understand the stalling effect, consider Figure~\ref{fig:stalling}, which shows the first iterations of Dykstra's algorithm with starting point in the red region. Note that the outcome of Dykstra's algorithm depends on the order of the sets $\mathcal{H}_i,\dots,\mathcal{H}_n$. In Figure~\ref{fig:stalling}, the algorithm starts by projecting onto the box and then onto the line. It can be seen that for the first 6 iterations\footnote{By one iteration we mean one cycle of $n$ projections here.}, Dykstra's algorithm returns the top left corner of the box (``stalling''). The authors also determine the exact number of iterations required to break free from the red region, and show that if the starting point is arbitrarily far to the left, the algorithm will need an arbitrarily large iteration number to break free from the red region.

% Figure demonstrating the stalling problem
\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Latex/Current Version/Figures/StallingRegionsHand.png}
        \caption{Line-box example with different regions that yield different convergence properties.}
        \label{fig:region}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Latex/Current Version/Figures/DifferentSequences.png}
        \caption{Stalling for the line-box example when $x_0$ is in the red region.}
        \label{fig:stalling}
    \end{subfigure}
    \caption{A demonstration of the stalling problem for a box and a line. Note how MAP applied to the same constraint sets would not result in any stalling: MAP follows the green line, and subsequently converges via the blue line path. Figure taken from~\cite{DYKSTRASTALLING}.}
    \label{fig:baushkeStall}
\end{figure}

\section{Main Result: Fast Forwarding Stalling Period}

By graphical examination of Figure~\ref{fig:baushkeStall} and formulae~\eqref{eq:dykstra:proj:poly}-\eqref{eq:km}, the following observations can be made:
\begin{enumerate}
\item The stalling period continues until one of the active half-spaces becomes inactive. This is a necessary condition.
\item During stalling, it holds that $x_m\equiv x_{m-n}\,\forall m$, so that every iteration, a constant vector $\Delta_m\eqdef x_{m-1}-x_m$ is added to $e_m$.
\end{enumerate}

These observations are formalised in the following theorem:
\begin{theorem}[Length of Stalling Period]\label{thm:nstall}
Suppose that at iteration $m$, the algorithm stalls, i.e.\ $x_{m+i}=x_{m-n+i}\,\forall i=0,\dots,n-1$. Then, the length of the stalling period is given by
\begin{align}\label{eq:nstall}
N_{stall}\eqdef\argmin_{i\in\mathcal{S}} \left\lfloor -k_{m-n+i} / \left(x_{m+i}^\Tr f_{[m+i]} - c_{[m+i]}\right)\right\rfloor,
\end{align}
where $\mathcal{S}\eqdef\set{i\in \mathcal{A}_m}{x_{m-n+i}^\Tr f_{[m+i]} - c_{[m+i]} < 0}$ and $\mathcal{A}_m\eqdef\set{i\in\lbrace 0,\dots,n-1\rbrace}{x_{m+i}+e_{m-n}\not\in\mathcal{H}_{[m+i]}}$.
\end{theorem}
\begin{proof}
Note that the existence of a finite number $N_{stall}$ is a consequence of the Boyle-Dykstra theorem. According to~\eqref{eq:dykstra:proj:poly}, the stalling period will terminate once one half-space becomes inactive, i.e. $x_{m+j}+e_{m-n+j}\not\in\mathcal{H}_{[m-n+j]}$ for some $[m-n+j]\in\mathcal{A}_m$. According to~\eqref{eq:km}, the only half-spaces that can become inactive are those for which $\delta k_{i} \eqdef x_{m+i}^\Tr f_{[m+i]} - c_{[m+i]} < 0$, which is, by definition, a constant during stalling. The length of the stalling period can therefore be obtained from choosing the smallest possible integer $N$ for which
\begin{align*}
k_{m-n+i}+N\left(x_{m+i}^\Tr f_{[m+i]} - c_{[m+i]}\right) < 0,\qquad [m+i]\in\mathcal{A}_m,
\end{align*}
which can be reformulated as in~\eqref{eq:nstall}.
\end{proof}

By Theorem~\eqref{thm:nstall}, the stalling period can be fast-forwarded by applying $N_{stall}$ constant increments to each auxiliary variable $k_m$ from~\eqref{eq:km}. This is illustrated for the line-box example in Figure~\ref{fig:stallingfixed}, where Figure~\ref{fig:stalling2} replicates the stalling situation from Figure~\ref{fig:baushkeStall} and the fast-forwarding is applied in Figure~\ref{fig:fastforwarding}. The first column illustrates the trajectories of the iterates and their distance to the Eucledian projection computed using an interior point method. The second column shows the half-space activity, where half-space 0 corresponds to the left side of the box, half-space 1 to the top side of the box, and half-space 2 to the line (down-facing normal). In Figure~\ref{fig:stalling2}, the algorithm stalls until iteration 16. From the second column, it can be seen that the algorithm stalls until half-space 0 becomes inactive. 

In Figure~\ref{fig:fastforwarding}, stalling is detected at iteration 1, upon which the algorithm is fast-forwarded using $N_{stall}=15$ iterations, where $N_{stall}$ is computed using Theorem~\ref{thm:nstall}. At iteration 3, the iterates of the modified algorithm arrive where the iterates of the original algorithm arrive at iteration 16. The iterates of the modified algorithm then become identical to those of the original algorithm.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Latex/Current Version/Figures/stalling_notfixed_situation_2.png}
        \caption{Replication of the stalling situation from Figure~\ref{fig:baushkeStall}.}
        \label{fig:stalling2}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Latex/Current Version/Figures/stalling_fixed_situation_2.png}
        \caption{Modified algorithm with fast-forwarding.}
        \label{fig:fastforwarding}
    \end{subfigure}
    \caption{Dykstra's algorithm applied to the line-box example from Figure~\ref{fig:baushkeStall}. The first column illustrates the trajectories of the iterates and their distance to the Eucledian projection computed using an interior point method. The second column shows the half-space activity, where half-space 0 corresponds to the left side of the box, half-space 1 to the top side of the box, and half-space 0 to the line (down-facing normal).}
    \label{fig:stallingfixed}
\end{figure}

\section{Additional Analysis: Control Perspective}

Based on~\cite{DYKSTRAPERKINS}, Dykstra's method can be split into two phases:
\begin{enumerate}
\item[(I)] An initial phase during which half-spaces are successively discarded, which includes stalling phases.
\item[(II)] A second phase during which alternating projections eventually converge to the global projection.
\end{enumerate}

Although the algorithm includes the non-linear projection operators onto each half-space, it can be simplified during both phases. Assume that at iteration $m$ all half-spaces are active and remain active until iteration $m+n$. In this case, each projection is a linear operator and~\eqref{eq:dykstra} (or~\eqref{eq:dykstra:proj:poly}) becomes:
\begin{subequations}\label{eq:dykstra20}
\begin{align}
x_{m+1}&=x_{m}+e_{m-n} - \left((x_{m}+e_{m-n})^\Tr f_{[m]} - c_{[m]}\right) f_{[m]},\label{eq:dykstra:proj20}\\
e_m&=\left((x_{m}+e_{m-n})^\Tr f_{[m]} - c_{[m]}\right) f_{[m]}\label{eq:dykstra:error20},
\end{align}
\end{subequations}
By considering that $e_m$ is always parallel to $f_{[m]}$, i.e. $e_m=k_mf_{[m]}$ as in~\eqref{eq:km}, we note that $((x_{m}+e_{m-n})^\Tr f_{[m]} - c_{[m]}) f_{[m]} = e_{m-n} + (x_{m}^\Tr f_{[m]} - c_{[m]}) f_{[m]}$. Moreover, it holds that $(x_{m}^\Tr f_{[m]}) f_{[m]} = f_{[m]} f_{[m]}^\Tr x_{m}$, and by defining
\begin{align*}
&F^\perp_{[m]}\eqdef I-f_{[m]}f_{[m]}^\Tr,
&\bar{b}_{[m]}\eqdef c_{[m]} f_{[m]},
\end{align*}
formulae~\eqref{eq:dykstra20} can be rewritten as
\begin{subequations}\label{eq:dykstra2}
\begin{align}
x_{m+1}&=F^\perp_{[m]}x_{m}+\bar{b}_{[m]}\reqdef \pi_{[m]}(x_{m}),\label{eq:dykstra:proj2}\\
e_m&=e_{m-n}+(I-F^\perp_{[m]})x_{m}-\bar{b}_{[m]} \reqdef e_{m-n}+\tilde{\pi}_{[m]}(x_{m}),\label{eq:dykstra:error2}
\end{align}
\end{subequations}
where $\tilde{\pi}_{[m]}(x_{m})=x_m-\pi_{[m]}(x_{m})$. Equations~\eqref{eq:dykstra2} hold as long as $x_m+e_{m-n}\not\in\mathcal{H}_{[m]}\forall m$, or equivalently
\begin{align}\label{eq:switch}
e_{m-n}^\Tr f_{[m]}+x_m^\Tr f_{[m]}-c_{[m]} > 0, \quad\forall m.
\end{align}
For the following analysis, change the indexing in~\eqref{eq:dykstra20} for $[m]=1,\dots,n-1$ as
\begin{align}\label{eq:index}
x_{m+1} \mapsto x_{k+1}^{[m]},\qquad x_{m} \mapsto x_{k}^{[m-1]},\qquad
e_{m-n} \mapsto e_{k}^{[m]},\qquad e_{m} \mapsto e_{k+1}^{[m]},
\end{align}
and for $[m]=0$ replace $x_{m} \mapsto x_{k}^{[m-1]}$ in~\eqref{eq:index} by $x_{m} \mapsto x_{k-1}^{[m-1]}$
so that~\eqref{eq:dykstra:proj2} becomes
\begin{align}\label{eq:dykstrareindex}
&x_{k+1}^{m}=
\begin{cases}
\pi_{m}(x_{k-1}^{n-1}) & \text{for } m=0,\\
\pi_{m}(x_{k}^{m-1}) & \text{otherwise}
\end{cases},
&e_{k+1}^{m}=e_{m-n}+
\begin{cases}
\tilde{\pi}_{m}(x_{k-1}^{n-1}) & \text{for } [m]=0,\\
\tilde{\pi}_{m}(x_{k}^{m-1}) & \text{otherwise},
\end{cases}
\end{align}
where now $m$ is assumed to be restricted to the range $m=0,\dots,n-1$ and $k=0,1,2,\dots$. Using the notation $\pi_i\circ\pi_j(x)=\pi_i(\pi_j(x))$, we expand~\eqref{eq:dykstrareindex} as
\begin{equation}
\begin{aligned}
x_{k+1}^0 &= \pi_0(x_k^{n-1}),\\
x_{k+1}^1 &= \pi_1\circ\pi_0(x_k^{n-1}),\\
&\,\,\vdots\\
x_{k+1}^{n-1} &= \pi_{n-1}\circ\pi_{n-2}\circ\dots\circ\pi_0(x_k^{n-1}),
\end{aligned}\label{eq:dykstra:proj3}
\end{equation}
and~\eqref{eq:dykstra:error2}
\begin{equation}
\begin{aligned}
e_{k+1}^0 &= e_k^0+\tilde{\pi}_0(x_k^{n-1}),\\
e_{k+1}^1 &= e_k^1+\tilde{\pi}_1\circ\pi_0(x_k^{n-1}),\\
&\,\,\vdots\\
e_{k+1}^{n-1} &= e_k^{n-1}+\pi_{n-1}\circ\pi_{n-2}\circ\dots\circ\pi_0(x_k^{n-1}).
\end{aligned}\label{eq:dykstra:error3}
\end{equation}
Replacing $x_k^{n-1}$ in~\eqref{eq:dykstra:proj3} and~\eqref{eq:dykstra:error3} by $x_k^{n-1}=\pi_{n-1}\circ\dots\circ\pi_{j-1}(x_k^{j})$ yields
\begin{subequations}
\begin{align}
x_{k+1}^i &= \underset{p=0}{\overset{n-1}{\circ}}\pi_{[i-p]}(x_{k}^i),\\
e_{k+1}^i &= e_k^i+\tilde{\pi}_i\circ\left(\underset{p=1}{\overset{n-1}{\circ}}\pi_{[i-p]}(x_{k}^i)\right)
= e_k^i+\underset{p=1}{\overset{n-1}{\circ}}\pi_{[i-p]}(x_{k}^i)-\underset{p=0}{\overset{n-1}{\circ}}\pi_{[i-p]}(x_{k}^i),
\end{align}\label{eq:dykstra4}
\end{subequations}
where $\underset{p=0}{\overset{n-1}{\circ}}\pi_{[i-p]}(x)=\pi_i\circ\dots\circ\pi_0\circ\pi_{n-1}\circ\dots\circ\pi_{i+1}(x)$. Substituting matrix notation for $\pi_{[i-p]}$ yields
\begin{subequations}
\begin{align}
x_{k+1}^i &= \left(\prod_{p=0}^{n-1}F^\perp_{[i-p]}\right)x_{k}^i+\sum_{p=0}^{n-1}\left(\prod_{l=0}^{n-2-p}F^\perp_{[i-l]}\right)\bar{b}_{[i-(n-1)+p]},\\
e_{k+1}^i &= e_k^i+(I-F^\perp_{i})\left(\prod_{p=1}^{n-1}F^\perp_{[i-p]}\right)x_{k}^i+(I-F^\perp_{i})\sum_{p=1}^{n-1}\left(\prod_{l=0}^{n-2-p}F^\perp_{[i-l]}\right)\bar{b}_{[i-(n-1)+p]} - \bar{b}_i.
\end{align}\label{eq:dykstra4}
\end{subequations}
and after gathering the matrices and constant vectors in $A_{x,i}$, $b_{x,i}$, $A_{e,i}$, and $b_{e,i}$:
\begin{subequations}
\begin{align}
&x_{k+1}^i = A_{x,i} x_{k}^i+b_{x,i},
&e_{k+1}^i = e_k^i+A_{e,i} x_{k}^i+b_{e,i}.
\end{align}
\end{subequations}
This is a standard LTI system with constant inputs that can be rewritten in explicit form as:
\begin{subequations}
\begin{align}
&x_{k}^i = A_{x,i}^k x_{0}^i+\sum_{p=0}^{k-1}A_{x,i}^p b_{x,i},
&e_{k}^i = e_0^i+\sum_{p=0}^{k-1}(A_{e,i} x_{p}^i+b_{e,i}).
\end{align}
\end{subequations}
We note that $A_{e,i}=\tilde{A}_i-A_{x,i}$ and $b_{e,i}=\tilde{b}_i-b_{x,i}$, where
\begin{align*}
&\tilde{A}_i=\prod_{p=1}^{n-1}F^\perp_{[i-p]},
&\tilde{b}_i=\sum_{p=1}^{n-1}\left(\prod_{l=0}^{n-2-p}F^\perp_{[i-l]}\right).
\end{align*}
Although the pair $(x_k^i, e_k^i)$ evolves independently of  $(x_k^j, e_k^j),\,j\neq i$, the half spaces are coupled through~\eqref{eq:switch}, which in the new notation reads as
\begin{align}\label{eq:switch2}
\begin{cases}
(e_k^{m})^\Tr f_{m}+(x_{k}^{n-1})^\Tr f_{m}-c_{m} > 0 & \text{for}\quad m=0,\\
(e_k^{m})^\Tr f_{m}+(x_{k+1}^{m-1})^\Tr f_{m}-c_{m} > 0 & \text{otherwise.}
\end{cases}
\end{align}

\newpage
For example, for $i=n-1$, the matrices and vectors obtained as
\begin{align*}
A_{x,n-1}&=F^\perp_{n-1} F^\perp_{n-2}\dots F^\perp_0,\\
b_{x,n-1}&=F^\perp_{n-1} F^\perp_{n-2}\dots F^\perp_1 c_0 f_0 + F^\perp_{n-1} F^\perp_{n-2}\dots F^\perp_2 c_1 f_1+\dots+F^\perp_{n-1}c_{n-2}f_{n-2}+c_{n-1} f_{n-1},\\
A_{e,n-1}&=(I-F^\perp_{n-1}) F^\perp_{n-2}\dots F^\perp_0,\\
b_{e,n-1}&=(I-F^\perp_{n-1}) F^\perp_{n-2}\dots F^\perp_1 c_0 f_0 + (I-F^\perp_{n-1}) F^\perp_{n-2}\dots F^\perp_2 c_1 f_1+\dots+(I-F^\perp_{n-1})c_{n-2}f_{n-2}-c_{n-1} f_{n-1}.
\end{align*}
For the remaining $i$, the matrices have similar structure but with products taken over cyclically permuted indices. Recalling that $F^\perp_{n-1}=I-f_{n-1}f_{n-1}^\Tr$, we note from setting $x_{k}^i=x_{k}^{i,\perp}+x_{k}^{i,||}$ with $x_{k}^{i,||}$ parallel to $f_{i}$:
\begin{align*}
x_{k+1}^{i,\perp}+x_{k+1}^{i,||} &= A_{x,i} (x_{k}^{i,\perp}+x_{k}^{i,||})+\underbrace{b_{x,i}}_{b_{x,i}^\perp + b_{x,i}^{||}},\\
&=A_{x,i} (x_{k}^{i,\perp}+x_{k}^{i,||})+b_{x,i}^\perp + b_{x,i}^{||},
\end{align*}
where $b_{x,i}^{||}=c_i f_i$. Considering that $A_{x,i} x \perp f_i$, we see that
\begin{align*}
&x_{k}^{i,||}=b_{x,i}^{||},
&x_{k+1}^{i,\perp}=A_{x,i}x_{k}^{i,\perp} + b_{x,i}^\perp+A_{x,i}b_{x,i}^{||}.
\end{align*}
In a possible steady state, 
\begin{align*}
(I-A_{x,i})x_{ss}^{i,\perp}=b_{x,i}^\perp+A_{x,i}b_{x,i}^{||}.
\end{align*}


\newpage
\begin{subequations}
\begin{align}
x_{k+1}^1 &= \pi_1(\pi_3(\pi_2(x_k^1))),\\
x_{k+1}^2 &= \pi_2(\pi_1(\pi_3(x_k^2))),\\
x_{k+1}^3 &= \pi_3(\pi_2(\pi_1(x_k^3))),\\
e_{k+1}^1 &= \tilde{\pi}_1(\pi_3(\pi_2(x_k^1))+e_k^1),\\
e_{k+1}^2 &= \tilde{\pi}_2(\pi_1(\pi_3(x_k^2))+e_k^2),\\
e_{k+1}^3 &= \tilde{\pi}_3(\pi_2(\pi_1(x_k^3))+e_k^3).
\end{align}\label{eq:dykstra4}
\end{subequations}
Clearly, since $\pi_i$ and $\tilde{\pi}_i$ are \textit{linear} operators, \eqref{eq:dykstra4} is of the form $\mathbf{x}_{k+1} = A\mathbf{x}_{k}$. Moreover, it can be seen that the pair $(x_k^i, e_k^i)$ evolves independently of  $(x_k^j, e_k^j),\,j\neq i$. However, the half spaces are coupled through~\eqref{eq:switch}, which in the new notation reads as
\begin{align}\label{eq:switch2}
\begin{cases}
(e_k^{m})^\Tr f_{m}+(x_{k+1}^{m-1})^\Tr f_{m}-c_{m} > 0 & \text{for}\quad m=1,\dots,n-1,\\
(e_k^{m})^\Tr f_{m}+(x_{k}^{n-1})^\Tr f_{m}-c_{m} > 0 & \text{for}\quad m=0.\\
\end{cases}
\end{align}
We compute $\pi_p(\pi_j(\pi_i(x)))$ as
\begin{equation}
\begin{aligned}
\pi_p(\pi_j(\pi_i(x))) 
&= \pi_p(\pi_j(x-(\alpha_{x,i} - c_i)f_i)))\\
&= \pi_p(x-(\alpha_{x,i}-c_i)f_i-(\alpha_{x,j} - (\alpha_{x,i}-c_i)\alpha_{i,j} -c_j)f_j)\\
&= x-(\alpha_{x,i}-c_i)f_i-(\alpha_{x,j} - (\alpha_{x,i}-c_i)\alpha_{x,j} -c_j)f_j\\
&\phantom{=}\,\,-(\alpha_{x,p}-(\alpha_{x,i}-c_i)\alpha_{i,p}-(\alpha_{x,j} - (\alpha_{x,i}-c_i)\alpha_{i,j} -c_j)\alpha_{j,p} -c_p)f_p\reqdef A_px+b_p,
\end{aligned}
\end{equation}
where $\alpha_{i,j}\eqdef f_i^\Tr f_j$ and
\begin{subequations}
\begin{align}
A_p&\eqdef I-f_i f_i^\Tr-f_j f_j^\Tr-f_p f_p^\Tr+\alpha_{i,j}f_j f_i^\Tr+(\alpha_{i,p} -\alpha_{i,j}\alpha_{j,p})f_p f_i^\Tr+\alpha_{j,p}f_pf_j^\Tr,\\
b_p&\eqdef c_i f_i+(c_j-c_i\alpha_{i,j})f_j+(c_p-c_i(\alpha_{i,p}-\alpha_{i,j}\alpha_{j,p})-c_j\alpha_{j,p})f_p.
\end{align}
\end{subequations}
Note that $A_p=A_{p\circ j\circ i}$, i.e. it depends on the order of $j$ and $i$. Similarly, $\tilde{\pi}_p(\pi_j(\pi_i(x))+e^p)$ is computed as
\begin{equation}
\begin{aligned}
\tilde{\pi}_p(\pi_j(\pi_i(x))+e^p)
&=e^p+\tilde{\pi}_p(\pi_j(\pi_i(x))),\\
&=e^p+x-\pi_p(\pi_j(\pi_i(x))),\\
&=e^p+(I-A_px-b_p.
\end{aligned}
\end{equation}
With these definitions, \eqref{eq:dykstra4} can be rewritten as
\begin{subequations}
\begin{align}
x_{k+1}^p&=A_p x_k^p +b_p,\\
e_{k+1}^p&=(I-A_p) x_k^p -b_p,
\end{align}
\end{subequations}
where $p\in\lbrace 1,2,3\rbrace$



\newpage
\subsection{OLD NOTATION}

Algorithm~\eqref{eq:dykstra2} can be interpreted as a \textit{linear, time-invariant dynamical system}. To see this, abbreviate~\eqref{eq:dykstra:proj2} and~\eqref{eq:dykstra:error2} as $x_{m+1}\eqdef\pi_{[m]}(x_{m}+e_{m-n})$ and $e_{m}\eqdef\tilde{\pi}_{[m]}(x_{m}+e_{m-n})$, respectively, and change the indexing as:
\begin{align}
x_{m+1} \mapsto x_{k+1}^{[m]},\qquad x_{m} \mapsto x_{k}^{[m-1]},\qquad
e_{m-n} \mapsto e_{k}^{[m]},\qquad e_{m} \mapsto e_{k+1}^{[m]}.
\end{align}
For simplicity, suppose that there are three half-spaces with iterates $x_k^1$, $x_k^2$, and $x_k^3$, and auxiliary variables $e_k^1$, $e_k^2$, and $e_k^3$. This allows to rewrite~\eqref{eq:dykstra:proj2} as
\begin{subequations}
\begin{align}
x_{k+1}^1 &= \pi_1(x_k^3+e_k^1),\\
x_{k+1}^2 &= \pi_2(\pi_1(x_k^3+e_k^1)+e_k^2),\\
x_{k+1}^3 &= \pi_3(\pi_2(\pi_1(x_k^3+e_k^1)+e_k^2)+e_k^3),
\end{align}\label{eq:dykstra:proj3}
\end{subequations}
and~\eqref{eq:dykstra:error2} as
\begin{subequations}
\begin{align}
e_{k+1}^1 &= \tilde{\pi}_1(x_k^3+e_k^1),\\
e_{k+1}^2 &= \tilde{\pi}_2(\pi_1(x_k^3+e_k^1)+e_k^2),\\
e_{k+1}^3 &= \tilde{\pi}_3(\pi_2(\pi_1(x_k^3+e_k^1)+e_k^2)+e_k^3).
\end{align}\label{eq:dykstra:error3}
\end{subequations}
Equations~\eqref{eq:dykstra:proj3} and~\eqref{eq:dykstra:error3} can be further manipulated by substituting $x_k^3=\pi_3(\pi_2(x_k^1+e_k^2)+e_k^3)$ for half-space 1 and and $x_k^3=\pi_3(x_k^2+e_k^3)$ for half-space 2:
\begin{subequations}
\begin{align}
x_{k+1}^1 &= \pi_1(\pi_3(\pi_2(x_k^1+e_k^2)+e_k^3)+e_k^1),\\
x_{k+1}^2 &= \pi_2(\pi_1(\pi_3(x_k^2+e_k^3)+e_k^1)+e_k^2),\\
x_{k+1}^3 &= \pi_3(\pi_2(\pi_1(x_k^3+e_k^1)+e_k^2)+e_k^3),\\
e_{k+1}^1 &= \tilde{\pi}_1(\pi_3(\pi_2(x_k^1+e_k^2)+e_k^3)+e_k^1),\\
e_{k+1}^2 &= \tilde{\pi}_2(\pi_1(\pi_3(x_k^2+e_k^3)+e_k^1)+e_k^2),\\
e_{k+1}^3 &= \tilde{\pi}_3(\pi_2(\pi_1(x_k^3+e_k^1)+e_k^2)+e_k^3).
\end{align}\label{eq:dykstra4}
\end{subequations}
Clearly, since $\pi_i$ and $\tilde{\pi}_i$ are \textit{linear} operators, \eqref{eq:dykstra4} is of the form $\mathbf{x}_{k+1} = A\mathbf{x}_{k}$, where
\begin{align*}
\mathbf{x}_{k+1}\eqdef\begin{pmatrix}x_{k+1}^1\\ x_{k+1}^2 \\ x_{k+1}^3\\ e_{k+1}^1\\ e_{k+1}^2\\ e_{k+1}^3\end{pmatrix}.
\end{align*}


%\section{Additional Ideas}
%
%For the polyhedral case, Dykstra's method can be split into two phases: An first phase of length $N_2$ during which some half-spaces become inactive, and a second phase during which the algorithm iterates between active half-spaces. Note that stalling can only occur during the first phase.
%
%For the first phase, the term $x_m^\Tr f_{[m]} - c_{[m]}$ in~\eqref{eq:km} seems to play an important role: the only way a half-space can become inactive is by obtaining a negative $x_m^\Tr f_{[m]} - c_{[m]}$. In~\cite{DYKSTRAPERKINS}, it is shown that
%\begin{align}
%x_{m+n} =\spanv{x_m, f_0,\dots,f_{n-1}}.
%\end{align}
%Moreover, by examination of~\eqref{eq:dykstra:proj:poly}, if $x_{m+n}+e_m\not\in\mathcal{H}_{[m]}$, we see that
%\begin{align}
%x_{m+n} =x_m + a_{[m]}f_{[m]}-\sum_{i\in \mathcal{I} \backslash [m]} a_i f_i,
%\end{align}
%where $a_i\geq 0$, $a_{[m]}\equiv k_m$, and $\mathcal{I}\eqdef \lbrace 0,\dots,n-1\rbrace$.

\bibliographystyle{plain}
\bibliography{master_bib_abbrev}
\end{document}